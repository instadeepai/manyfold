data:
  common:
    # MSA specific features in alphafold.yaml
    num_recycle: 3 # has to be equal to model_config.model.num_recycle
    supervised_features: [all_atom_mask, all_atom_positions, resolution, atom14_gt_positions,
      atom14_alt_gt_positions, atom14_atom_exists, atom14_gt_exists, atom14_alt_gt_exists,
      atom14_atom_is_ambiguous, backbone_affine_tensor, backbone_affine_mask, chi_angles,
      alt_chi_angles, chi_mask, rigidgroups_gt_frames, rigidgroups_alt_gt_frames,
      rigidgroups_group_exists, rigidgroups_group_is_ambiguous, rigidgroups_gt_exists,
      pdb_cluster_size]
    template_features: [template_all_atom_positions, template_sum_probs, template_aatype,
      template_all_atom_masks, template_domain_names]
    unsupervised_features: [aatype, residue_index, sequence, msa, domain_name, num_alignments,
      seq_length, between_segment_residues, deletion_matrix]
    use_supervised: true
    use_templates: false
    crop_size: -1
    batch_size: 1
  eval:
    feat:
      aatype: [num residues placeholder]
      all_atom_mask: [num residues placeholder, null]
      all_atom_positions: [num residues placeholder, null, null]
      alt_chi_angles: [num residues placeholder, null, null] # this field has been added since we are computing
                      # sin and cos functions of the chi angles in the input pipeline
                      # now (see edit in folding.py)
      atom14_alt_gt_exists: [num residues placeholder, null]
      atom14_alt_gt_positions: [num residues placeholder, null, null]
      atom14_atom_exists: [num residues placeholder, null]
      atom14_atom_is_ambiguous: [num residues placeholder, null]
      atom14_gt_exists: [num residues placeholder, null]
      atom14_gt_positions: [num residues placeholder, null, null]
      atom37_atom_exists: [num residues placeholder, null]
      backbone_affine_mask: [num residues placeholder]
      backbone_affine_tensor: [num residues placeholder, null]
      bert_mask: [msa placeholder, num residues placeholder]
      chi_angles: [num residues placeholder, null, null]
      chi_mask: [num residues placeholder, null] # this field has been added since we are computing
                      # sin and cos functions of the chi angles in the input pipeline
                      # now (see edit in folding.py)
      extra_deletion_value: [extra msa placeholder, num residues placeholder]
      extra_has_deletion: [extra msa placeholder, num residues placeholder]
      extra_msa: [extra msa placeholder, num residues placeholder]
      extra_msa_mask: [extra msa placeholder, num residues placeholder]
      extra_msa_row_mask: [extra msa placeholder]
      msa_feat: [msa placeholder, num residues placeholder, null]
      msa_mask: [msa placeholder, num residues placeholder]
      msa_row_mask: [msa placeholder]
      pseudo_beta: [num residues placeholder, null]
      pseudo_beta_mask: [num residues placeholder]
      random_crop_to_size_seed: [null]
      residue_index: [num residues placeholder]
      residx_atom14_to_atom37: [num residues placeholder, null]
      residx_atom37_to_atom14: [num residues placeholder, null]
      resolution: []
      rigidgroups_alt_gt_frames: [num residues placeholder, null, null]
      rigidgroups_group_exists: [num residues placeholder, null]
      rigidgroups_group_is_ambiguous: [num residues placeholder, null]
      rigidgroups_gt_exists: [num residues placeholder, null]
      rigidgroups_gt_frames: [num residues placeholder, null, null]
      seq_length: []
      seq_mask: [num residues placeholder]
      target_feat: [num residues placeholder, null]
      template_aatype: [num templates placeholder, num residues placeholder]
      template_all_atom_masks: [num templates placeholder, num residues placeholder,
        null]
      template_all_atom_positions: [num templates placeholder, num residues placeholder,
        null, null]
      template_backbone_affine_mask: [num templates placeholder, num residues placeholder]
      template_backbone_affine_tensor: [num templates placeholder, num residues placeholder,
        null]
      template_mask: [num templates placeholder]
      template_pseudo_beta: [num templates placeholder, num residues placeholder,
        null]
      template_pseudo_beta_mask: [num templates placeholder, num residues placeholder]
      template_sum_probs: [num templates placeholder, null]
      true_msa: [msa placeholder, num residues placeholder]
      use_clamped_fape: []
    fixed_size: true
    # MSA features in alphafold_config.yaml
    max_templates: 4
    num_ensemble: 1
    subsample_templates: false # We want top templates.
  train:
    # Overwrites "eval" defaults if is_training=True.
    # MSA features in alphafold_config.yaml
    subsample_templates: true  # We want to sample templates.
    unclamped_fape_fraction: 0.1 # None to skip this behaviour.

model:
  embeddings_and_evoformer:
    evoformer:
      msa_column_attention: {
        dropout_rate: 0.0,
        gating: true,
        num_head: 8,
        orientation: per_column,
        shared_dropout: true,
        subbatch_size: null,
        subbatch_size_global: 128
      }
      msa_row_attention_with_pair_bias: {
        dropout_rate: 0.15,
        gating: true,
        num_head: 8,
        orientation: per_row,
        shared_dropout: true,
        subbatch_size: null
      }
      msa_row_attention_with_pair_bias_extra_msa: {
        dropout_rate: 0.15,
        gating: true,
        num_head: 8,
        orientation: per_row,
        shared_dropout: true,
        subbatch_size: 128
      }
      msa_transition: {
        dropout_rate: 0.0,
        num_intermediate_factor: 4,
        orientation: per_row,
        shared_dropout: true
      }
      outer_product_mean: {
        chunk_size: 128,
        dropout_rate: 0.0,
        first: false,
        num_outer_channel: 32,
        orientation: per_row,
        shared_dropout: true
      }
      pair_transition: {
        dropout_rate: 0.0,
        num_intermediate_factor: 4,
        orientation: per_row,
        shared_dropout: true
      }
      triangle_attention_ending_node: {
        dropout_rate: 0.25,
        gating: true,
        num_head: 4,
        orientation: per_column,
        shared_dropout: true
      }
      triangle_attention_starting_node: {
        dropout_rate: 0.25,
        gating: true,
        num_head: 4,
        orientation: per_row,
        shared_dropout: true
      }
      triangle_multiplication_incoming: {
        dropout_rate: 0.25,
        equation: 'kjc,kic->ijc',
        num_intermediate_channel: 128,
        orientation: per_row,
        shared_dropout: true
      }
      triangle_multiplication_outgoing: {
      dropout_rate: 0.25,
      equation: 'ikc,jkc->ijc',
      num_intermediate_channel: 128,
      orientation: per_row,
      shared_dropout: true
    }
    evoformer_num_block: 48
    extra_msa_channel: 64
    extra_msa_stack_num_block: 4
    max_relative_feature: 32
    msa_channel: 256
    pair_channel: 128
    prev_pos: {
      max_bin: 20.75,
      min_bin: 3.25,
      num_bins: 15
    }
    recycle_features: true
    recycle_pos: true
    seq_channel: 384
    template:
      attention: {
        gating: false,
        key_dim: 64,
        num_head: 4,
        value_dim: 64
      }
      dgram_features: {
        max_bin: 50.75,
        min_bin: 3.25,
        num_bins: 39
      }
      embed_torsion_angles: false
      enabled: false
      max_templates: 4
      subbatch_size: 128
      template_pair_stack:
        num_block: 2
        pair_transition: {
          dropout_rate: 0.0,
          num_intermediate_factor: 2,
          orientation: per_row,
          shared_dropout: true
        }
        triangle_attention_ending_node: {
          dropout_rate: 0.25,
          gating: true,
          key_dim: 64,
          num_head: 4,
          orientation: per_column,
          shared_dropout: true,
          value_dim: 64
        }
        triangle_attention_starting_node: {
          dropout_rate: 0.25,
          gating: true,
          key_dim: 64,
          num_head: 4,
          orientation: per_row,
          shared_dropout: true,
          value_dim: 64
        }
        triangle_multiplication_incoming: {
          dropout_rate: 0.25,
          equation: 'kjc,kic->ijc',
          num_intermediate_channel: 64,
          orientation: per_row,
          shared_dropout: true
        }
        triangle_multiplication_outgoing: {
          dropout_rate: 0.25,
          equation: 'ikc,jkc->ijc',
          num_intermediate_channel: 64,
          orientation: per_row,
          shared_dropout: true
        }
      use_template_unit_vector: false
  global_config: {
    deterministic: false,
    plmfold_mode: false,
    subbatch_size: 4,
    use_remat: false,
    zero_init: false,
  }
  heads:
    distogram: {
      first_break: 2.3125,
      last_break: 21.6875,
      num_bins: 64,
      weight: 0.3,
    }
    experimentally_resolved: {
      filter_by_resolution: true,
      max_resolution: 3.0,
      min_resolution: 0.1,
      weight: 0.0,
    }
    masked_msa: {
      num_output: 23,
      weight: 2.0,
    }
    predicted_aligned_error: { # <--> TM-score prediction
      # `num_bins - 1` bins uniformly space the
      # [0, max_error_bin A] range.
      # The final bin covers [max_error_bin A, +infty]
      # 31A gives bins with 0.5A width.
      filter_by_resolution: true,
      max_error_bin: 31.0,
      max_resolution: 3.0,
      min_resolution: 0.1,
      num_bins: 64,
      num_channels: 128,
      weight: 0.0
    }
    predicted_lddt: {
      filter_by_resolution: true,
      max_resolution: 3.0,
      min_resolution: 0.1,
      num_bins: 50,
      num_channels: 128,
      weight: 0.01
    }
    structure_module:
      angle_norm_weight: 0.01
      chi_weight: 0.5
      clash_overlap_tolerance: 1.5
      compute_in_graph_metrics: true
      dropout: 0.1
      fape: {
        clamp_distance: 10.0,
        clamp_type: relu,
        loss_unit_distance: 10.0
      }
      num_channel: 384
      num_head: 12
      num_layer: 8
      num_layer_in_transition: 3
      num_point_qk: 4
      num_point_v: 8
      num_scalar_qk: 16
      num_scalar_v: 16
      position_scale: 10.0
      sidechain: {
        atom_clamp_distance: 10.0,
        length_scale: 10.0,
        num_channel: 128,
        num_residual_block: 2,
        weight_frac: 0.5
      }
      structural_violation_loss_weight: 0.0
      violation_tolerance_factor: 12.0
      weight: 1.0
  num_recycle: ${..data.common.num_recycle}

train:
  num_steps: 250000
  continue_from_last_checkpoint: true
  use_remat: true # -> config.model.global_config.use_remat = True
  skip_nonfinite_updates: true
  invalid_paramaters_action: null
  mixed_precision: {
    use_half: true,
    scale_type: NoOp,  # Dynamic, Static, NoOp
    scale_value: 32768.0
  }
  checkpointing: {
    checkpoint_dir: ./,
    checkpoint_every_n_steps: 250,
    keep_num: 10,
    keep_last_num: 5
  }
  optimizer: {
    lr: 0.001,
    warm_up_n_steps: 1000.0,
    b1: 0.9,
    b2: 0.999,
    lr_decay: 0.95,
    lr_decay_after_n_steps: 50000.0,
    clip_global_grad_norm: 0.1,
    num_grad_acc_steps: 1
  }
  validate: {
    max_batches: -1,
    validate_every_n_steps: 250, # -1 to turn validation off.
    validate_at_start: false,
    ewa_decay: 0.999,
    ewa_debias: true,
  }
